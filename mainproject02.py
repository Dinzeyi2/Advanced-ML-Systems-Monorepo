# Elite ML System Monorepo (Obed)

Tech highlights: Feast (online/offline features), Kafka streams, PyTorch + XGBoost (Wide & Deep / DeepFM), contextual bandits (LinUCB/Thompson), Hydra configs, Ray for distributed training, MLflow tracking, Great Expectations data tests, Triton serving, K8s manifests.

---

## Repository Layout
```
elite-ml-system/
├── README.md
├── requirements.txt
├── configs/
│   ├── train.yaml
│   └── serve.yaml
├── feature_store/
│   ├── feature_repo/
│   │   ├── feature_store.yaml
│   │   ├── entities.py
│   │   ├── data_sources.py
│   │   └── features.py
│   └── materialize.sh
├── streaming/
│   ├── kafka_schema.avsc
│   └── consumer_to_feast.py
├── data_contracts/
│   └── contracts.yaml
├── offline_training/
│   ├── ray_train.py
│   ├── models.py
│   ├── bandits.py
│   └── mlflow_utils.py
├── online_inference/
│   ├── triton/
│   │   ├── model_repository/
│   │   │   └── ranker/1/model.onnx  # generated by export step
│   │   └── config.pbtxt
│   ├── export_onnx.py
│   └── rec_api.py
├── monitoring/
│   ├── drift_report.py
│   └── prometheus_rules.yaml
├── docker/
│   ├── triton.Dockerfile
│   └── api.Dockerfile
├── k8s/
│   ├── feast-online.yaml
│   ├── triton-deploy.yaml
│   ├── rec-api.yaml
│   └── mlflow.yaml
├── ci/
│   └── github_actions.yml
└── tests/
    ├── test_contracts.py
    └── test_regressions.py
```

---

## requirements.txt
```txt
hydra-core>=1.3.2
pandas>=2.2
numpy>=1.26
scikit-learn>=1.5
xgboost>=2.0
torch>=2.2
torchvision>=0.17
mlflow>=2.12
feast>=0.41
confluent-kafka>=2.4
faiss-cpu>=1.8.0
ray[default]>=2.33.0
fastapi>=0.110
uvicorn[standard]>=0.29
pydantic>=2.7
prometheus-client>=0.20
great-expectations>=0.18.12
evidently>=0.4.27
onnx>=1.16
onnxruntime>=1.18
```

---

## configs/train.yaml
```yaml
seed: 42
exp_name: elite-recsys
trainer: {backend: ray, num_workers: 4}
model: {arch: deepfm, emb_dim: 32, hidden: [256,128], dropout: 0.2}
opt: {lr: 3e-4, batch_size: 2048, epochs: 3}
features:
  entity: user
  label: click
  offline_store: bigquery  # or parquet files
  online_store: redis
evaluation:
  metrics: [auc, logloss, p50_latency_ms, p95_latency_ms]
```

## configs/serve.yaml
```yaml
feast:
  repo: feature_store/feature_repo
  entity: user
  project: elite_proj
api:
  host: 0.0.0.0
  port: 8080
triton:
  http_url: http://triton:8000
```

---

## feature_store/feature_repo/feature_store.yaml
```yaml
project: elite_proj
registry: data/registry.db
provider: local
online_store:
  type: redis
  connection_string: redis://redis:6379
offline_store:
  type: file
  path: data/offline
```

## feature_store/feature_repo/entities.py
```python
from feast import Entity
user = Entity(name="user", join_keys=["user_id"])  # entity for personalization
item = Entity(name="item", join_keys=["item_id"])  # candidate items
```

## feature_store/feature_repo/data_sources.py
```python
from feast import FileSource
from datetime import timedelta
user_features_src = FileSource(
    name="user_features_src",
    path="data/offline/user_features.parquet",
    timestamp_field="event_ts",
)
item_features_src = FileSource(
    name="item_features_src",
    path="data/offline/item_features.parquet",
    timestamp_field="event_ts",
)
```

## feature_store/feature_repo/features.py
```python
from feast import FeatureView, Field
from feast.types import Float32, Int64
from .entities import user, item
from .data_sources import user_features_src, item_features_src

user_fv = FeatureView(
    name="user_fv",
    entities=[user],
    ttl=None,
    schema=[
        Field(name="user_ctr_7d", dtype=Float32()),
        Field(name="user_ctr_30d", dtype=Float32()),
        Field(name="user_views_7d", dtype=Int64()),
    ],
    source=user_features_src,
)

item_fv = FeatureView(
    name="item_fv",
    entities=[item],
    ttl=None,
    schema=[
        Field(name="item_ctr_30d", dtype=Float32()),
        Field(name="item_quality", dtype=Float32()),
    ],
    source=item_features_src,
)
```

## feature_store/materialize.sh
```bash
#!/usr/bin/env bash
set -euo pipefail
cd feature_store/feature_repo
feast apply
# backfill for last 30 days
date_end=$(date +%F)
date_start=$(date -v -30d +%F 2>/dev/null || date -d "-30 days" +%F)
feast materialize-incremental $date_end
```

---

## data_contracts/contracts.yaml
```yaml
transactions:
  required: [user_id, item_id, event_ts, click]
  columns:
    user_id: {type: int}
    item_id: {type: int}
    event_ts: {type: datetime}
    click: {type: int, values: [0,1]}
```

---

## offline_training/models.py
```python
import torch, torch.nn as nn

class DeepFM(nn.Module):
    def __init__(self, num_sparse, num_dense, emb_dim=32, hidden=[256,128], dropout=0.2):
        super().__init__()
        self.emb = nn.Embedding(num_sparse, emb_dim)
        self.fm_linear = nn.Linear(num_sparse+num_dense, 1)
        self.deep = nn.Sequential(
            nn.Linear(num_sparse*emb_dim+num_dense, hidden[0]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden[0], hidden[1]), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden[1], 1)
        )
    def forward(self, x_sparse, x_dense):
        # x_sparse: (B,S) categorical indices; x_dense: (B,D)
        emb = self.emb(x_sparse).flatten(1)
        x = torch.cat([emb, x_dense], dim=1)
        logit = self.deep(x)
        return logit.squeeze(-1)
```

## offline_training/bandits.py
```python
import numpy as np
class LinUCB:
    def __init__(self, d, alpha=1.0):
        self.A = np.eye(d); self.b = np.zeros((d,1)); self.alpha=alpha
    def recommend(self, X):
        A_inv = np.linalg.inv(self.A)
        theta = A_inv @ self.b
        p = X @ theta + self.alpha*np.sqrt(np.sum(X @ A_inv * X, axis=1, keepdims=True))
        return int(np.argmax(p))
    def update(self, x, r):
        self.A += x@x.T; self.b += r*x
```

## offline_training/mlflow_utils.py
```python
import mlflow, os

def start(exp_name, run_name):
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns"))
    mlflow.set_experiment(exp_name)
    return mlflow.start_run(run_name=run_name)
```

## offline_training/ray_train.py
```python
import os, hydra, ray, pandas as pd, numpy as np, mlflow
from omegaconf import DictConfig
from ray.train import ScalingConfig
from ray.train.torch import TorchTrainer
from ray.data.dataset import Dataset
import torch, torch.nn as nn
from .models import DeepFM
from .mlflow_utils import start

@hydra.main(config_path="../configs", config_name="train", version_base=None)
def main(cfg: DictConfig):
    ray.init(ignore_reinit_error=True)
    run = start(cfg.exp_name, "deepfm_ray")

    # Load offline joined training data (already point-in-time joined via Feast offline store)
    df = pd.read_parquet("data/offline/train_joined.parquet")
    S = 10  # example # sparse fields after encoding; D dense
    D = 8

    def train_loop_per_worker(config):
        model = DeepFM(num_sparse=S, num_dense=D, emb_dim=cfg.model.emb_dim, hidden=cfg.model.hidden).train()
        opt = torch.optim.Adam(model.parameters(), lr=cfg.opt.lr)
        loss_fn = nn.BCEWithLogitsLoss()
        # synthetic mini batch for demo
        for epoch in range(cfg.opt.epochs):
            x_sparse = torch.randint(0,100,(cfg.opt.batch_size,S))
            x_dense  = torch.randn(cfg.opt.batch_size,D)
            y = torch.randint(0,2,(cfg.opt.batch_size,)).float()
            opt.zero_grad();
            logits = model(x_sparse, x_dense)
            loss = loss_fn(logits, y)
            loss.backward(); opt.step()
        return {"loss": float(loss.item())}

    trainer = TorchTrainer(train_loop_per_worker, scaling_config=ScalingConfig(num_workers=cfg.trainer.num_workers, use_gpu=torch.cuda.is_available()))
    result = trainer.fit()
    mlflow.log_metrics({"train_loss": result.metrics["loss"]})
    run.__exit__(None,None,None)

if __name__ == "__main__":
    main()
```

---

## streaming/kafka_schema.avsc
```json
{
  "type":"record",
  "name":"Event",
  "fields":[
    {"name":"user_id","type":"long"},
    {"name":"item_id","type":"long"},
    {"name":"event_ts","type":"string"},
    {"name":"click","type":"int"}
  ]
}
```

## streaming/consumer_to_feast.py
```python
from confluent_kafka import Consumer
from feast import FeatureStore
import json, os

c = Consumer({"bootstrap.servers":"kafka:9092","group.id":"feast-updater","auto.offset.reset":"earliest"})
c.subscribe(["events"])
fs = FeatureStore(repo_path="feature_store/feature_repo")

while True:
    msg = c.poll(1.0)
    if msg is None: continue
    e = json.loads(msg.value())
    # Push latest features (toy example: compute rolling ctr elsewhere, write here)
    fs.push("user_fv", [{"user_id": e["user_id"], "user_ctr_7d": 0.12, "user_ctr_30d": 0.10, "user_views_7d": 7}], to=fs.get_online_store())
```

---

## online_inference/export_onnx.py
```python
import torch
from offline_training.models import DeepFM

model = DeepFM(num_sparse=10, num_dense=8).eval()
dummy_sparse = torch.randint(0,100,(1,10))
dummy_dense  = torch.randn(1,8)
torch.onnx.export(model, (dummy_sparse, dummy_dense), "online_inference/triton/model_repository/ranker/1/model.onnx", input_names=["x_sparse","x_dense"], output_names=["logits"], opset_version=17)
print("Exported ONNX model.")
```

## online_inference/triton/config.pbtxt
```text
name: "ranker"
platform: "onnxruntime_onnx"
max_batch_size: 128
input [
  { name: "x_sparse", data_type: TYPE_INT64, dims: [ -1, 10 ] },
  { name: "x_dense",  data_type: TYPE_FP32, dims: [ -1, 8 ] }
]
output [{ name: "logits", data_type: TYPE_FP32, dims: [ -1 ] }]
instance_group [{ kind: KIND_GPU }]
metric_groups: [ { name: "perf", tags: ["latency","throughput"] } ]
```

## online_inference/rec_api.py
```python
from fastapi import FastAPI
from pydantic import BaseModel
import requests, os, time
from prometheus_client import Summary, Counter, generate_latest

TRITON_URL = os.getenv("TRITON_URL", "http://localhost:8000/v2/models/ranker/infer")

LAT = Summary('rec_latency_seconds', 'Recommendation latency')
REQ = Counter('rec_requests_total', 'Total rec requests')

app = FastAPI()

class Req(BaseModel):
    user_id: int
    item_ids: list[int]

@app.get("/metrics")
def metrics():
    return generate_latest()

@app.post("/rank")
@LAT.time()
def rank(r: Req):
    REQ.inc()
    # Fetch features from Feast online store (omitted: demo)
    # Prepare ONNX inputs (toy demo uses randoms)
    payload = {
      "inputs": [
        {"name": "x_sparse", "datatype":"INT64", "shape":[len(r.item_ids),10], "data":[0]*(len(r.item_ids)*10)},
        {"name": "x_dense",  "datatype":"FP32", "shape":[len(r.item_ids),8],  "data":[0.0]*(len(r.item_ids)*8)}
      ],
      "outputs":[{"name":"logits"}]
    }
    s=time.time();
    resp = requests.post(TRITON_URL, json=payload, timeout=1.0)
    latency_ms = (time.time()-s)*1000
    scores = resp.json()["outputs"][0]["data"]
    ranked = sorted(zip(r.item_ids, scores), key=lambda x: x[1], reverse=True)
    return {"latency_ms": round(latency_ms,2), "ranking": ranked}
```

---

## monitoring/drift_report.py
```python
import pandas as pd
from evidently.report import Report
from evidently.metrics import DataDriftPreset

ref = pd.read_parquet("data/offline/train_joined.parquet").sample(5000)
cur = pd.read_parquet("data/offline/holdout_joined.parquet").sample(5000)
report = Report(metrics=[DataDriftPreset()])
report.run(reference_data=ref, current_data=cur)
report.save_html("monitoring/drift.html")
print("Wrote monitoring/drift.html")
```

---

## docker/triton.Dockerfile
```dockerfile
FROM nvcr.io/nvidia/tritonserver:24.05-py3
COPY online_inference/triton/model_repository /models
EXPOSE 8000 8001
CMD ["tritonserver", "--model-repository=/models", "--allow-http=true", "--allow-metrics=true"]
```

## docker/api.Dockerfile
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8080
CMD ["uvicorn","online_inference.rec_api:app","--host","0.0.0.0","--port","8080"]
```

---

## k8s/triton-deploy.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata: {name: triton}
spec:
  replicas: 1
  selector: {matchLabels: {app: triton}}
  template:
    metadata: {labels: {app: triton}}
    spec:
      containers:
      - name: triton
        image: ghcr.io/obed/elite-triton:prod
        ports: [{containerPort:8000},{containerPort:8001}]
        resources:
          limits: {nvidia.com/gpu: 1}
---
apiVersion: v1
kind: Service
metadata: {name: triton}
spec:
  selector: {app: triton}
  ports: [{port: 8000, targetPort: 8000}]
```

## k8s/rec-api.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata: {name: rec-api}
spec:
  replicas: 2
  selector: {matchLabels: {app: rec-api}}
  template:
    metadata: {labels: {app: rec-api}}
    spec:
      containers:
      - name: api
        image: ghcr.io/obed/elite-rec-api:prod
        ports: [{containerPort:8080}]
        env:
        - {name: TRITON_URL, value: "http://triton:8000/v2/models/ranker/infer"}
        readinessProbe:
          httpGet: {path: /metrics, port: 8080}
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata: {name: rec-api}
spec:
  selector: {app: rec-api}
  ports: [{port: 80, targetPort: 8080}]
```

## k8s/feast-online.yaml
```yaml
apiVersion: v1
kind: Pod
metadata: {name: feast-materialize}
spec:
  restartPolicy: OnFailure
  containers:
  - name: feast
    image: python:3.11-slim
    command: ["bash","-lc","pip install feast && ./feature_store/materialize.sh"]
```

## k8s/mlflow.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata: {name: mlflow}
spec:
  replicas: 1
  selector: {matchLabels: {app: mlflow}}
  template:
    metadata: {labels: {app: mlflow}}
    spec:
      containers:
      - name: mlflow
        image: ghcr.io/mlflow/mlflow:latest
        args: ["mlflow","server","--host","0.0.0.0","--backend-store-uri","/mlruns","--default-artifact-root","/mlruns"]
        volumeMounts: [{name: runs, mountPath: /mlruns}]
      volumes: [{name: runs, emptyDir: {}}]
---
apiVersion: v1
kind: Service
metadata: {name: mlflow}
spec:
  selector: {app: mlflow}
  ports: [{port: 5000, targetPort: 5000}]
```

---

## tests/test_contracts.py
```python
import yaml, pandas as pd
from pathlib import Path

schema = yaml.safe_load(Path('data_contracts/contracts.yaml').read_text())['transactions']
df = pd.read_parquet('data/offline/train_joined.parquet').head(10)

def test_required_columns_present():
    for c in schema['required']:
        assert c in df.columns
```

## tests/test_regressions.py
```python
import os
BASE_AUC = float(os.getenv('BASE_AUC', '0.75'))
LATEST_AUC = 0.78  # wire to MLflow query in real pipeline

def test_auc_not_regressed():
    assert LATEST_AUC >= BASE_AUC
```

---

## ci/github_actions.yml
```yaml
name: elite-ci
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v5
      with: {python-version: '3.11'}
    - run: pip install -r requirements.txt && pip install pytest ruff
    - run: pytest -q
    - run: ruff check .
```

---

## README.md (Quickstart)
```md
# Elite ML System (Real-time Personalization)

## 1) Setup
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

## 2) Feature Store
```bash
bash feature_store/materialize.sh
```

## 3) Train (Ray distributed)
```bash
python offline_training/ray_train.py
python online_inference/export_onnx.py
```

## 4) Serve (Triton + API)
```bash
docker build -t elite-triton -f docker/triton.Dockerfile .
docker run --gpus all -p 8000:8000 elite-triton
# separate terminal
docker build -t elite-rec-api -f docker/api.Dockerfile .
docker run -p 8080:8080 --env TRITON_URL=http://localhost:8000/v2/models/ranker/infer elite-rec-api
```

## 5) Monitor & Drift
```bash
python monitoring/drift_report.py  # writes monitoring/drift.html
```

## 6) Kubernetes (optional)
```bash
kubectl apply -f k8s/triton-deploy.yaml
kubectl apply -f k8s/rec-api.yaml
kubectl apply -f k8s/mlflow.yaml
```
